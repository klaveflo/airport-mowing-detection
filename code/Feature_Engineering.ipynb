{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf062a4",
   "metadata": {},
   "source": [
    "# Feature Engineering \n",
    "\n",
    "**Author:** Florian Klaver  \n",
    "\n",
    "In this Jupyter Notebook the features for the modelling are created. This also contains the calculation of vegetation indices and sampling of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cbbfe",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c5dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from scipy.ndimage import binary_erosion\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac79885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "sentinel_dir = '../data/Sentinel_CH/'\n",
    "masks_dir = '../data/ground_truth_masks/'\n",
    "features_output_dir = '../data/features_all_indices/' \n",
    "\n",
    "final_samples_output_path = '../data/training_samples_all_indices.csv'\n",
    "cloud_stats_output_path = '../data/cloud_masking_stats.csv'\n",
    "\n",
    "os.makedirs(features_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1528bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "CLOUD_BAND = 13\n",
    "CLOUD_THRESHOLD = 30  # Percent\n",
    "N_POSITIVE_SAMPLES = 50000  # Mowed\n",
    "N_NEGATIVE_SAMPLES = 50000  # Not mowed\n",
    "\n",
    "# Time windows for temporal matching\n",
    "\n",
    "# BEFORE_FAR: 9-20 days before mowing. This image will be used for the negative samples (not mowed)\n",
    "# BEFORE_NEAR: 3-8 days before mowing\n",
    "# AFTER: 1-7 days after mowing (at least 1 because we don't know the exact time of the mowing event).\n",
    "# BEFORE_FAR and BEFORE_NEAR will be used to create negative samples (not mowed)\n",
    "# BEFORE_NEAR and AFTER will be used to create positive samples (mowed)\n",
    "\n",
    "BEFORE_FAR_MIN = 9\n",
    "BEFORE_FAR_MAX = 20\n",
    "BEFORE_NEAR_MIN = 3\n",
    "BEFORE_NEAR_MAX = 8\n",
    "AFTER_MIN = 1\n",
    "AFTER_MAX = 7\n",
    "\n",
    "# Feature List\n",
    "FEATURE_NAMES = [\n",
    "    # Indices AFTER (State) - 5 features\n",
    "    'ndvi_after', 'evi_after', 'savi_after', 'gndvi_after', 'ndii_after',\n",
    "    \n",
    "    # Indices DIFF (Change) - 5 features\n",
    "    'ndvi_diff', 'evi_diff', 'savi_diff', 'gndvi_diff', 'ndii_diff',\n",
    "    \n",
    "    # Raw Band DIFFs (Change) - 5 features\n",
    "    'blue_diff', 'green_diff', 'red_diff', 'nir_diff', 'swir_diff',\n",
    "\n",
    "    # Raw Bands AFTER (Optional state) - 5 features\n",
    "    'blue_after', 'green_after', 'red_after', 'nir_after', 'swir_after'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164bd21",
   "metadata": {},
   "source": [
    "Note: Other time windows and different approaches for searching the image date combinations than the chosen time windows were also tested. For example just choosing the two images before the mowing event (without time restrictions), a combination of both (using time constraints only for AFTER and BFORE_FAR) and other day limits for the given parameters. This chosen combination proofed to work the best in terms of matches found and data quality which directly influenced model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdab22",
   "metadata": {},
   "source": [
    "---\n",
    "### Temporal matching of Sentinel-2 and Ground Truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352ce153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 227 ground truth records and 201 Sentinel-2 images.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2eb6bd050d64ef98cb3fbbb5def54cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding Matches:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92 matches with 3 valid images.\n"
     ]
    }
   ],
   "source": [
    "# Load Ground truth\n",
    "masks_overview = pd.read_csv('../data/ground_truth_masks/masks_overview.csv')\n",
    "masks_overview['date'] = pd.to_datetime(masks_overview['date'])\n",
    "\n",
    "\n",
    "# Load Sentinel-2 files\n",
    "sentinel_files = sorted(glob.glob(os.path.join(sentinel_dir, '*.tif')))\n",
    "sentinel_dates = []\n",
    "\n",
    "for f in sentinel_files:\n",
    "    filename = os.path.basename(f).replace('.tif', '')\n",
    "    try:\n",
    "        date = pd.to_datetime(filename)\n",
    "        sentinel_dates.append({'file': f, 'date': date, 'filename': filename})\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Cannot parse date: {filename}. Error: {e}\")\n",
    "\n",
    "sentinel_df = pd.DataFrame(sentinel_dates)\n",
    "print(f\"Loaded {len(masks_overview)} ground truth records and {len(sentinel_df)} Sentinel-2 images.\")\n",
    "\n",
    "\n",
    "# Initialize list to store matches\n",
    "matches = []\n",
    "\n",
    "# For each ground truth mask, find matching Sentinel-2 images\n",
    "for idx, row in tqdm(masks_overview.iterrows(), total=len(masks_overview), desc=\"Finding Matches\"):\n",
    "    event_date = row['date']\n",
    "   \n",
    "\n",
    "    after_window = sentinel_df[\n",
    "        (sentinel_df['date'] >= event_date + timedelta(days=AFTER_MIN)) &\n",
    "        (sentinel_df['date'] <= event_date + timedelta(days=AFTER_MAX))\n",
    "    ]\n",
    "\n",
    "    before_near_window = sentinel_df[\n",
    "        (sentinel_df['date'] >= event_date - timedelta(days=BEFORE_NEAR_MAX)) &\n",
    "        (sentinel_df['date'] <= event_date - timedelta(days=BEFORE_NEAR_MIN))\n",
    "    ]\n",
    "\n",
    "    before_far_window = sentinel_df[\n",
    "        (sentinel_df['date'] >= event_date - timedelta(days=BEFORE_FAR_MAX)) &\n",
    "        (sentinel_df['date'] <= event_date - timedelta(days=BEFORE_FAR_MIN))\n",
    "    ]\n",
    "\n",
    "   # If there is at least one image in each window, take the closest ones\n",
    "    if len(after_window) > 0 and len(before_near_window) > 0 and len(before_far_window) > 0:\n",
    "        img_after = after_window.iloc[0]\n",
    "        img_before_near = before_near_window.iloc[-1]\n",
    "        img_before_far = before_far_window.iloc[-1]\n",
    "\n",
    "        matches.append({\n",
    "            'event_date': event_date,\n",
    "            'event_date_str': row['date_str'],\n",
    "            'mask_file': row['filename'],\n",
    "            'before_far_file': img_before_far['filename'] + '.tif',\n",
    "            'before_near_file': img_before_near['filename'] + '.tif',\n",
    "            'after_file': img_after['filename'] + '.tif',\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "# Save matches to DataFrame for further processing\n",
    "matches_df = pd.DataFrame(matches)\n",
    "print(f\"Found {len(matches_df)} matches with 3 valid images.\")\n",
    "\n",
    "# Save matches dataframe to csv for model assessment Pipeline\n",
    "matches_df.to_csv('../data/temporal_matches.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f70befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "event_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "event_date_str",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mask_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "before_far_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "before_near_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "after_file",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "4e7ba7de-4465-4806-9879-d0bf9500c70e",
       "rows": [
        [
         "0",
         "2019-05-09 00:00:00",
         "20190509",
         "mask_20190509.tif",
         "2019-04-30.tif",
         "2019-05-05.tif",
         "2019-05-10.tif"
        ],
        [
         "1",
         "2019-05-16 00:00:00",
         "20190516",
         "mask_20190516.tif",
         "2019-05-07.tif",
         "2019-05-12.tif",
         "2019-05-17.tif"
        ],
        [
         "2",
         "2019-05-23 00:00:00",
         "20190523",
         "mask_20190523.tif",
         "2019-05-12.tif",
         "2019-05-20.tif",
         "2019-05-25.tif"
        ],
        [
         "3",
         "2019-05-30 00:00:00",
         "20190530",
         "mask_20190530.tif",
         "2019-05-20.tif",
         "2019-05-27.tif",
         "2019-06-01.tif"
        ],
        [
         "4",
         "2019-05-31 00:00:00",
         "20190531",
         "mask_20190531.tif",
         "2019-05-20.tif",
         "2019-05-27.tif",
         "2019-06-01.tif"
        ],
        [
         "5",
         "2019-06-05 00:00:00",
         "20190605",
         "mask_20190605.tif",
         "2019-05-27.tif",
         "2019-06-01.tif",
         "2019-06-06.tif"
        ],
        [
         "6",
         "2019-06-06 00:00:00",
         "20190606",
         "mask_20190606.tif",
         "2019-05-27.tif",
         "2019-06-01.tif",
         "2019-06-09.tif"
        ],
        [
         "7",
         "2019-06-16 00:00:00",
         "20190616",
         "mask_20190616.tif",
         "2019-06-06.tif",
         "2019-06-11.tif",
         "2019-06-19.tif"
        ],
        [
         "8",
         "2019-06-17 00:00:00",
         "20190617",
         "mask_20190617.tif",
         "2019-06-06.tif",
         "2019-06-14.tif",
         "2019-06-19.tif"
        ],
        [
         "9",
         "2019-06-18 00:00:00",
         "20190618",
         "mask_20190618.tif",
         "2019-06-09.tif",
         "2019-06-14.tif",
         "2019-06-19.tif"
        ],
        [
         "10",
         "2019-06-19 00:00:00",
         "20190619",
         "mask_20190619.tif",
         "2019-06-09.tif",
         "2019-06-16.tif",
         "2019-06-21.tif"
        ],
        [
         "11",
         "2019-06-20 00:00:00",
         "20190620",
         "mask_20190620.tif",
         "2019-06-11.tif",
         "2019-06-16.tif",
         "2019-06-21.tif"
        ],
        [
         "12",
         "2019-06-23 00:00:00",
         "20190623",
         "mask_20190623.tif",
         "2019-06-14.tif",
         "2019-06-19.tif",
         "2019-06-24.tif"
        ],
        [
         "13",
         "2019-06-26 00:00:00",
         "20190626",
         "mask_20190626.tif",
         "2019-06-16.tif",
         "2019-06-21.tif",
         "2019-06-29.tif"
        ],
        [
         "14",
         "2019-06-27 00:00:00",
         "20190627",
         "mask_20190627.tif",
         "2019-06-16.tif",
         "2019-06-24.tif",
         "2019-06-29.tif"
        ],
        [
         "15",
         "2019-07-02 00:00:00",
         "20190702",
         "mask_20190702.tif",
         "2019-06-21.tif",
         "2019-06-29.tif",
         "2019-07-04.tif"
        ],
        [
         "16",
         "2019-07-03 00:00:00",
         "20190703",
         "mask_20190703.tif",
         "2019-06-24.tif",
         "2019-06-29.tif",
         "2019-07-04.tif"
        ],
        [
         "17",
         "2019-07-04 00:00:00",
         "20190704",
         "mask_20190704.tif",
         "2019-06-24.tif",
         "2019-07-01.tif",
         "2019-07-06.tif"
        ],
        [
         "18",
         "2019-07-15 00:00:00",
         "20190715",
         "mask_20190715.tif",
         "2019-07-06.tif",
         "2019-07-11.tif",
         "2019-07-16.tif"
        ],
        [
         "19",
         "2019-07-16 00:00:00",
         "20190716",
         "mask_20190716.tif",
         "2019-07-06.tif",
         "2019-07-11.tif",
         "2019-07-19.tif"
        ],
        [
         "20",
         "2019-07-18 00:00:00",
         "20190718",
         "mask_20190718.tif",
         "2019-07-09.tif",
         "2019-07-14.tif",
         "2019-07-19.tif"
        ],
        [
         "21",
         "2019-07-19 00:00:00",
         "20190719",
         "mask_20190719.tif",
         "2019-07-09.tif",
         "2019-07-16.tif",
         "2019-07-21.tif"
        ],
        [
         "22",
         "2019-07-24 00:00:00",
         "20190724",
         "mask_20190724.tif",
         "2019-07-14.tif",
         "2019-07-21.tif",
         "2019-07-26.tif"
        ],
        [
         "23",
         "2019-07-25 00:00:00",
         "20190725",
         "mask_20190725.tif",
         "2019-07-16.tif",
         "2019-07-21.tif",
         "2019-07-26.tif"
        ],
        [
         "24",
         "2019-08-14 00:00:00",
         "20190814",
         "mask_20190814.tif",
         "2019-08-05.tif",
         "2019-08-10.tif",
         "2019-08-15.tif"
        ],
        [
         "25",
         "2019-08-21 00:00:00",
         "20190821",
         "mask_20190821.tif",
         "2019-08-10.tif",
         "2019-08-18.tif",
         "2019-08-23.tif"
        ],
        [
         "26",
         "2019-08-22 00:00:00",
         "20190822",
         "mask_20190822.tif",
         "2019-08-13.tif",
         "2019-08-18.tif",
         "2019-08-23.tif"
        ],
        [
         "27",
         "2019-08-23 00:00:00",
         "20190823",
         "mask_20190823.tif",
         "2019-08-13.tif",
         "2019-08-20.tif",
         "2019-08-25.tif"
        ],
        [
         "28",
         "2019-08-24 00:00:00",
         "20190824",
         "mask_20190824.tif",
         "2019-08-15.tif",
         "2019-08-20.tif",
         "2019-08-25.tif"
        ],
        [
         "29",
         "2019-08-27 00:00:00",
         "20190827",
         "mask_20190827.tif",
         "2019-08-18.tif",
         "2019-08-23.tif",
         "2019-08-28.tif"
        ],
        [
         "30",
         "2019-08-28 00:00:00",
         "20190828",
         "mask_20190828.tif",
         "2019-08-18.tif",
         "2019-08-25.tif",
         "2019-08-30.tif"
        ],
        [
         "31",
         "2019-09-02 00:00:00",
         "20190902",
         "mask_20190902.tif",
         "2019-08-23.tif",
         "2019-08-30.tif",
         "2019-09-04.tif"
        ],
        [
         "32",
         "2019-09-10 00:00:00",
         "20190910",
         "mask_20190910.tif",
         "2019-08-30.tif",
         "2019-09-07.tif",
         "2019-09-12.tif"
        ],
        [
         "33",
         "2019-09-11 00:00:00",
         "20190911",
         "mask_20190911.tif",
         "2019-09-02.tif",
         "2019-09-07.tif",
         "2019-09-12.tif"
        ],
        [
         "34",
         "2019-09-12 00:00:00",
         "20190912",
         "mask_20190912.tif",
         "2019-09-02.tif",
         "2019-09-09.tif",
         "2019-09-14.tif"
        ],
        [
         "35",
         "2019-09-13 00:00:00",
         "20190913",
         "mask_20190913.tif",
         "2019-09-04.tif",
         "2019-09-09.tif",
         "2019-09-14.tif"
        ],
        [
         "36",
         "2019-09-18 00:00:00",
         "20190918",
         "mask_20190918.tif",
         "2019-09-09.tif",
         "2019-09-14.tif",
         "2019-09-19.tif"
        ],
        [
         "37",
         "2019-09-19 00:00:00",
         "20190919",
         "mask_20190919.tif",
         "2019-09-09.tif",
         "2019-09-14.tif",
         "2019-09-22.tif"
        ],
        [
         "38",
         "2019-09-27 00:00:00",
         "20190927",
         "mask_20190927.tif",
         "2019-09-17.tif",
         "2019-09-24.tif",
         "2019-09-29.tif"
        ],
        [
         "39",
         "2019-09-30 00:00:00",
         "20190930",
         "mask_20190930.tif",
         "2019-09-19.tif",
         "2019-09-27.tif",
         "2019-10-02.tif"
        ],
        [
         "40",
         "2019-10-03 00:00:00",
         "20191003",
         "mask_20191003.tif",
         "2019-09-24.tif",
         "2019-09-29.tif",
         "2019-10-04.tif"
        ],
        [
         "41",
         "2020-05-26 00:00:00",
         "20200526",
         "mask_20200526.tif",
         "2020-05-14.tif",
         "2020-05-21.tif",
         "2020-05-29.tif"
        ],
        [
         "42",
         "2020-05-27 00:00:00",
         "20200527",
         "mask_20200527.tif",
         "2020-05-14.tif",
         "2020-05-24.tif",
         "2020-05-29.tif"
        ],
        [
         "43",
         "2020-05-28 00:00:00",
         "20200528",
         "mask_20200528.tif",
         "2020-05-19.tif",
         "2020-05-24.tif",
         "2020-05-29.tif"
        ],
        [
         "44",
         "2020-06-01 00:00:00",
         "20200601",
         "mask_20200601.tif",
         "2020-05-21.tif",
         "2020-05-29.tif",
         "2020-06-03.tif"
        ],
        [
         "45",
         "2020-06-02 00:00:00",
         "20200602",
         "mask_20200602.tif",
         "2020-05-24.tif",
         "2020-05-29.tif",
         "2020-06-03.tif"
        ],
        [
         "46",
         "2020-06-21 00:00:00",
         "20200621",
         "mask_20200621.tif",
         "2020-06-03.tif",
         "2020-06-18.tif",
         "2020-06-23.tif"
        ],
        [
         "47",
         "2020-06-22 00:00:00",
         "20200622",
         "mask_20200622.tif",
         "2020-06-13.tif",
         "2020-06-18.tif",
         "2020-06-23.tif"
        ],
        [
         "48",
         "2020-06-23 00:00:00",
         "20200623",
         "mask_20200623.tif",
         "2020-06-13.tif",
         "2020-06-18.tif",
         "2020-06-25.tif"
        ],
        [
         "49",
         "2020-06-24 00:00:00",
         "20200624",
         "mask_20200624.tif",
         "2020-06-13.tif",
         "2020-06-18.tif",
         "2020-06-25.tif"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 92
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>event_date_str</th>\n",
       "      <th>mask_file</th>\n",
       "      <th>before_far_file</th>\n",
       "      <th>before_near_file</th>\n",
       "      <th>after_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>20190509</td>\n",
       "      <td>mask_20190509.tif</td>\n",
       "      <td>2019-04-30.tif</td>\n",
       "      <td>2019-05-05.tif</td>\n",
       "      <td>2019-05-10.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>20190516</td>\n",
       "      <td>mask_20190516.tif</td>\n",
       "      <td>2019-05-07.tif</td>\n",
       "      <td>2019-05-12.tif</td>\n",
       "      <td>2019-05-17.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-23</td>\n",
       "      <td>20190523</td>\n",
       "      <td>mask_20190523.tif</td>\n",
       "      <td>2019-05-12.tif</td>\n",
       "      <td>2019-05-20.tif</td>\n",
       "      <td>2019-05-25.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>20190530</td>\n",
       "      <td>mask_20190530.tif</td>\n",
       "      <td>2019-05-20.tif</td>\n",
       "      <td>2019-05-27.tif</td>\n",
       "      <td>2019-06-01.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>20190531</td>\n",
       "      <td>mask_20190531.tif</td>\n",
       "      <td>2019-05-20.tif</td>\n",
       "      <td>2019-05-27.tif</td>\n",
       "      <td>2019-06-01.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>20230905</td>\n",
       "      <td>mask_20230905.tif</td>\n",
       "      <td>2023-08-24.tif</td>\n",
       "      <td>2023-09-01.tif</td>\n",
       "      <td>2023-09-06.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>20230906</td>\n",
       "      <td>mask_20230906.tif</td>\n",
       "      <td>2023-08-24.tif</td>\n",
       "      <td>2023-09-01.tif</td>\n",
       "      <td>2023-09-08.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>20230907</td>\n",
       "      <td>mask_20230907.tif</td>\n",
       "      <td>2023-08-24.tif</td>\n",
       "      <td>2023-09-01.tif</td>\n",
       "      <td>2023-09-08.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2023-09-14</td>\n",
       "      <td>20230914</td>\n",
       "      <td>mask_20230914.tif</td>\n",
       "      <td>2023-09-01.tif</td>\n",
       "      <td>2023-09-11.tif</td>\n",
       "      <td>2023-09-16.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>20230915</td>\n",
       "      <td>mask_20230915.tif</td>\n",
       "      <td>2023-09-06.tif</td>\n",
       "      <td>2023-09-11.tif</td>\n",
       "      <td>2023-09-16.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_date  event_date_str          mask_file before_far_file  \\\n",
       "0  2019-05-09        20190509  mask_20190509.tif  2019-04-30.tif   \n",
       "1  2019-05-16        20190516  mask_20190516.tif  2019-05-07.tif   \n",
       "2  2019-05-23        20190523  mask_20190523.tif  2019-05-12.tif   \n",
       "3  2019-05-30        20190530  mask_20190530.tif  2019-05-20.tif   \n",
       "4  2019-05-31        20190531  mask_20190531.tif  2019-05-20.tif   \n",
       "..        ...             ...                ...             ...   \n",
       "87 2023-09-05        20230905  mask_20230905.tif  2023-08-24.tif   \n",
       "88 2023-09-06        20230906  mask_20230906.tif  2023-08-24.tif   \n",
       "89 2023-09-07        20230907  mask_20230907.tif  2023-08-24.tif   \n",
       "90 2023-09-14        20230914  mask_20230914.tif  2023-09-01.tif   \n",
       "91 2023-09-15        20230915  mask_20230915.tif  2023-09-06.tif   \n",
       "\n",
       "   before_near_file      after_file  \n",
       "0    2019-05-05.tif  2019-05-10.tif  \n",
       "1    2019-05-12.tif  2019-05-17.tif  \n",
       "2    2019-05-20.tif  2019-05-25.tif  \n",
       "3    2019-05-27.tif  2019-06-01.tif  \n",
       "4    2019-05-27.tif  2019-06-01.tif  \n",
       "..              ...             ...  \n",
       "87   2023-09-01.tif  2023-09-06.tif  \n",
       "88   2023-09-01.tif  2023-09-08.tif  \n",
       "89   2023-09-01.tif  2023-09-08.tif  \n",
       "90   2023-09-11.tif  2023-09-16.tif  \n",
       "91   2023-09-11.tif  2023-09-16.tif  \n",
       "\n",
       "[92 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(matches_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86084028",
   "metadata": {},
   "source": [
    "### Calculate relevant indices and Cloud masking\n",
    "\n",
    "In this section the necessary vegetation indices (used as features for the model) are calculated and pixels with high cloud probability receive NAN-values so they don't generate wrong values and interfere with the models learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d61d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All functions to calculate relevant vegetation indices\n",
    "\n",
    "def calculate_ndvi(nir, red):\n",
    "    \"\"\"Calculates NDVI, handle division by zero\"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ndvi = (nir - red) / (nir + red)\n",
    "    ndvi[np.isinf(ndvi)] = np.nan \n",
    "    return ndvi\n",
    "\n",
    "def calculate_evi(nir, red, blue):\n",
    "    \"\"\"Enhanced Vegetation Index\"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1))\n",
    "    evi[np.isinf(evi)] = np.nan \n",
    "    return evi\n",
    "\n",
    "def calculate_savi(nir, red, L=0.5):\n",
    "    \"\"\"Soil Adjusted Vegetation Index (L=0.5 is standard)\"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        savi = ((nir - red) / (nir + red + L)) * (1 + L)\n",
    "    savi[np.isinf(savi)] = np.nan\n",
    "    return savi\n",
    "\n",
    "def calculate_gndvi(nir, green):\n",
    "    \"\"\"Green Normalized Difference Vegetation Index\"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        gndvi = (nir - green) / (nir + green)\n",
    "    gndvi[np.isinf(gndvi)] = np.nan\n",
    "    return gndvi\n",
    "\n",
    "def calculate_ndii(nir, swir):\n",
    "    \"\"\"Normalized Difference Infrared Index (for water content)\"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ndii = (nir - swir) / (nir + swir)\n",
    "    ndii[np.isinf(ndii)] = np.nan\n",
    "    return ndii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b4b183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98d0f6edace402fa2ef3d0c49f0e2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Feature Rasters:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize list to store cloud statistics\n",
    "cloud_stats = []\n",
    "\n",
    "# For each match, create feature rasters\n",
    "for idx, row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Creating Feature Rasters\"):\n",
    "    \n",
    "    # Relevant paths\n",
    "    before_far_path = os.path.join(sentinel_dir, row['before_far_file'])\n",
    "    before_near_path = os.path.join(sentinel_dir, row['before_near_file'])\n",
    "    after_path = os.path.join(sentinel_dir, row['after_file'])\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(before_far_path) as src_far, \\\n",
    "             rasterio.open(before_near_path) as src_near, \\\n",
    "             rasterio.open(after_path) as src_after:\n",
    "            \n",
    "            meta = src_near.meta.copy()\n",
    "            \n",
    "            # Read all bands\n",
    "            # 'Far' bands (t-2)\n",
    "            blue_far = src_far.read(1).astype(float)\n",
    "            green_far = src_far.read(2).astype(float)\n",
    "            red_far = src_far.read(3).astype(float)\n",
    "            nir_far = src_far.read(7).astype(float)\n",
    "            swir_far = src_far.read(10).astype(float)\n",
    "            \n",
    "            # 'Near' bands (t-1)\n",
    "            blue_near = src_near.read(1).astype(float)\n",
    "            green_near = src_near.read(2).astype(float)\n",
    "            red_near = src_near.read(3).astype(float)\n",
    "            nir_near = src_near.read(7).astype(float)\n",
    "            swir_near = src_near.read(10).astype(float)\n",
    "            \n",
    "            # 'After' bands (t) \n",
    "            blue_after = src_after.read(1).astype(float)\n",
    "            green_after = src_after.read(2).astype(float)\n",
    "            red_after = src_after.read(3).astype(float)\n",
    "            nir_after = src_after.read(7).astype(float)\n",
    "            swir_after = src_after.read(10).astype(float)\n",
    "            \n",
    "            # Read cloud masks\n",
    "            cloud_prob_far = src_far.read(CLOUD_BAND).astype(float)\n",
    "            cloud_prob_near = src_near.read(CLOUD_BAND).astype(float)\n",
    "            cloud_prob_after = src_after.read(CLOUD_BAND).astype(float)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping match {row['event_date_str']} due to error loading files: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Process POSITIVE (Mowing) Pair (Near vs. After)\n",
    "    cloud_mask_pos = (cloud_prob_near > CLOUD_THRESHOLD) | (cloud_prob_after > CLOUD_THRESHOLD)\n",
    "    \n",
    "    # Calculate 'before' (near) indices\n",
    "    ndvi_before_pos = calculate_ndvi(nir_near, red_near)\n",
    "    evi_before_pos = calculate_evi(nir_near, red_near, blue_near)\n",
    "    savi_before_pos = calculate_savi(nir_near, red_near)\n",
    "    gndvi_before_pos = calculate_gndvi(nir_near, green_near)\n",
    "    ndii_before_pos = calculate_ndii(nir_near, swir_near)\n",
    "    \n",
    "    # Calculate 'after' indices\n",
    "    ndvi_after_pos = calculate_ndvi(nir_after, red_after)\n",
    "    evi_after_pos = calculate_evi(nir_after, red_after, blue_after)\n",
    "    savi_after_pos = calculate_savi(nir_after, red_after)\n",
    "    gndvi_after_pos = calculate_gndvi(nir_after, green_after)\n",
    "    ndii_after_pos = calculate_ndii(nir_after, swir_after)\n",
    "    \n",
    "    # Stack features\n",
    "    features_positive = np.stack([\n",
    "        # Indices After\n",
    "        ndvi_after_pos, evi_after_pos, savi_after_pos, gndvi_after_pos, ndii_after_pos,\n",
    "        # Indices Diff\n",
    "        ndvi_after_pos - ndvi_before_pos, \n",
    "        evi_after_pos - evi_before_pos, \n",
    "        savi_after_pos - savi_before_pos, \n",
    "        gndvi_after_pos - gndvi_before_pos, \n",
    "        ndii_after_pos - ndii_before_pos,\n",
    "        # Band Diffs\n",
    "        blue_after - blue_near, green_after - green_near, red_after - red_near, nir_after - nir_near, swir_after - swir_near,\n",
    "        # Bands After\n",
    "        blue_after, green_after, red_after, nir_after, swir_after\n",
    "    ])\n",
    "    # Apply cloud mask\n",
    "    features_positive[:, cloud_mask_pos] = np.nan\n",
    "    \n",
    "    # Save POSITIVE features\n",
    "    output_path_pos = os.path.join(features_output_dir, f'features_POSITIVE_{row[\"event_date_str\"]}.tif')\n",
    "    meta.update({'count': features_positive.shape[0], 'dtype': 'float32', 'nodata': np.nan})\n",
    "    with rasterio.open(output_path_pos, 'w', **meta) as dst:\n",
    "        dst.write(features_positive.astype('float32'))\n",
    "\n",
    "    # Process NEGATIVE (No Mowing) Pair (Far vs. Near)\n",
    "    cloud_mask_neg = (cloud_prob_far > CLOUD_THRESHOLD) | (cloud_prob_near > CLOUD_THRESHOLD)\n",
    "    \n",
    "    # Calculate 'before' (far) indices\n",
    "    ndvi_before_neg = calculate_ndvi(nir_far, red_far)\n",
    "    evi_before_neg = calculate_evi(nir_far, red_far, blue_far)\n",
    "    savi_before_neg = calculate_savi(nir_far, red_far)\n",
    "    gndvi_before_neg = calculate_gndvi(nir_far, green_far)\n",
    "    ndii_before_neg = calculate_ndii(nir_far, swir_far)\n",
    "    \n",
    "    # Calculate 'after' (near) indices\n",
    "    ndvi_after_neg = calculate_ndvi(nir_near, red_near)\n",
    "    evi_after_neg = calculate_evi(nir_near, red_near, blue_near)\n",
    "    savi_after_neg = calculate_savi(nir_near, red_near)\n",
    "    gndvi_after_neg = calculate_gndvi(nir_near, green_near)\n",
    "    ndii_after_neg = calculate_ndii(nir_near, swir_near)\n",
    "    \n",
    "    # Stack features\n",
    "    features_negative = np.stack([\n",
    "        # Indices After\n",
    "        ndvi_after_neg, evi_after_neg, savi_after_neg, gndvi_after_neg, ndii_after_neg,\n",
    "        # Indices Diff\n",
    "        ndvi_after_neg - ndvi_before_neg, \n",
    "        evi_after_neg - evi_before_neg, \n",
    "        savi_after_neg - savi_before_neg, \n",
    "        gndvi_after_neg - gndvi_before_neg, \n",
    "        ndii_after_neg - ndii_before_neg,\n",
    "        # Band Diffs\n",
    "        blue_near - blue_far, green_near - green_far, red_near - red_far, nir_near - nir_far, swir_near - swir_far,\n",
    "        # Bands After\n",
    "        blue_near, green_near, red_near, nir_near, swir_near\n",
    "    ])\n",
    "    # Apply cloud mask\n",
    "    features_negative[:, cloud_mask_neg] = np.nan\n",
    "    \n",
    "    # Save NEGATIVE features\n",
    "    output_path_neg = os.path.join(features_output_dir, f'features_NEGATIVE_{row[\"event_date_str\"]}.tif')\n",
    "    meta.update({'count': features_negative.shape[0], 'dtype': 'float32', 'nodata': np.nan})\n",
    "    with rasterio.open(output_path_neg, 'w', **meta) as dst:\n",
    "        dst.write(features_negative.astype('float32'))\n",
    "\n",
    "    # Statistics\n",
    "    n_total = cloud_mask_pos.size\n",
    "    n_cloudy_pos = np.sum(cloud_mask_pos)\n",
    "    pct_cloudy_pos = (n_cloudy_pos / n_total) * 100\n",
    "    n_cloudy_neg = np.sum(cloud_mask_neg)\n",
    "    pct_cloudy_neg = (n_cloudy_neg / n_total) * 100\n",
    "    \n",
    "    # Store cloud stats\n",
    "    cloud_stats.append({\n",
    "        'event_date_str': row['event_date_str'],\n",
    "        'pct_cloudy_pos': pct_cloudy_pos,\n",
    "        'pct_cloudy_neg': pct_cloudy_neg\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceabe13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_stats_df = pd.DataFrame(cloud_stats)\n",
    "cloud_stats_df.to_csv(cloud_stats_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13268fe9",
   "metadata": {},
   "source": [
    "---\n",
    "### Balanced Sampling\n",
    "Here points are sampled from the full set of negative and positive features. It is made sure to only sample pixels which lie below the set cloud probability threshold of 30% (= not NAN). Additionally a buffer of 10 meters (which equals 1 pixel) is introduced spanning the edges in all directions to account for the spatial shift (inaccuracy) given by the sentinel-2 imagery. By using this feature it is made sure to only sample more in the center of the mowed areas, lowering the chance of getting still faulty pixels (of streets, buildings or generally outside the actually mowed area) caused by the spatial shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bf8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting 50000 positive and 50000 negative samples.\n",
      "  → ~543 positive samples per match (from mask==1)\n",
      "  → ~543 negative samples per match (from mask==1)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4926bc5fe9844208bcc1bda1635474ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling (Pos/Neg):   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate samples per match\n",
    "samples_per_match_positive = max(1, N_POSITIVE_SAMPLES // len(matches_df))\n",
    "samples_per_match_negative = max(1, N_NEGATIVE_SAMPLES // len(matches_df))\n",
    "\n",
    "print(f\"Targeting {N_POSITIVE_SAMPLES} positive and {N_NEGATIVE_SAMPLES} negative samples.\")\n",
    "print(f\"  → ~{samples_per_match_positive} positive samples per match (from mask==1)\")\n",
    "print(f\"  → ~{samples_per_match_negative} negative samples per match (from mask==1)\\n\")\n",
    "\n",
    "# Initialize list to store all samples\n",
    "all_samples = []\n",
    "\n",
    "# For each match, sample positive and negative samples\n",
    "for idx, match_row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Sampling (Pos/Neg)\"):\n",
    "    \n",
    "    event_date = pd.to_datetime(match_row['event_date'])\n",
    "    \n",
    "    features_pos_path = os.path.join(features_output_dir, f'features_POSITIVE_{match_row[\"event_date_str\"]}.tif')\n",
    "    features_neg_path = os.path.join(features_output_dir, f'features_NEGATIVE_{match_row[\"event_date_str\"]}.tif')\n",
    "    mask_path = os.path.join(masks_dir, match_row['mask_file'])\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(features_pos_path) as feat_pos_src, \\\n",
    "             rasterio.open(features_neg_path) as feat_neg_src, \\\n",
    "             rasterio.open(mask_path) as mask_src:\n",
    "            \n",
    "            # Read the data\n",
    "            features_pos = feat_pos_src.read()\n",
    "            features_neg = feat_neg_src.read()\n",
    "            mask = mask_src.read(1)\n",
    "            \n",
    "            # Apply Negative Buffer (Erosion)\n",
    "            # --> Shrink the mask by 1 pixel (10m) to avoid edge effects/shift\n",
    "            mask_eroded = binary_erosion(mask == 1, structure=np.ones((3,3)), iterations=1)\n",
    "            \n",
    "            # Use the ERODED mask as final polygon mask\n",
    "            polygon_mask = (mask_eroded == 1)\n",
    "            \n",
    "            # Check if there are any pixels left after erosion\n",
    "            if np.sum(polygon_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Find Valid Pixels (Robust Sampling)\n",
    "            # Positive: Inside Eroded Polygon AND Not NaN\n",
    "            valid_mask_pos = polygon_mask & ~np.isnan(features_pos[0])\n",
    "            valid_indices_pos = np.where(valid_mask_pos)\n",
    "            n_valid_pos = len(valid_indices_pos[0])\n",
    "            \n",
    "            # Negative: Inside Eroded Polygon AND Not NaN\n",
    "            valid_mask_neg = polygon_mask & ~np.isnan(features_neg[0])\n",
    "            valid_indices_neg = np.where(valid_mask_neg)\n",
    "            n_valid_neg = len(valid_indices_neg[0])\n",
    "\n",
    "            # Extract Positive Samples\n",
    "            if n_valid_pos > 0:\n",
    "                n_to_sample = min(samples_per_match_positive, n_valid_pos)\n",
    "                choices = np.random.choice(n_valid_pos, size=n_to_sample, replace=False)\n",
    "                \n",
    "                rows = valid_indices_pos[0][choices]\n",
    "                cols = valid_indices_pos[1][choices]\n",
    "                \n",
    "                for r, c in zip(rows, cols):\n",
    "                    feature_values = features_pos[:, r, c]\n",
    "                    x, y = feat_pos_src.xy(r, c)\n",
    "                    all_samples.append({\n",
    "                        'x': x, 'y': y, 'event_date': event_date,\n",
    "                        'match_id': idx, 'label': 1,\n",
    "                        **{FEATURE_NAMES[i]: feature_values[i] for i in range(len(FEATURE_NAMES))}\n",
    "                    })\n",
    "\n",
    "            # Extract Negative Samples\n",
    "            if n_valid_neg > 0:\n",
    "                n_to_sample = min(samples_per_match_negative, n_valid_neg)\n",
    "                choices = np.random.choice(n_valid_neg, size=n_to_sample, replace=False)\n",
    "                \n",
    "                rows = valid_indices_neg[0][choices]\n",
    "                cols = valid_indices_neg[1][choices]\n",
    "                \n",
    "                for r, c in zip(rows, cols):\n",
    "                    feature_values = features_neg[:, r, c]\n",
    "                    x, y = feat_neg_src.xy(r, c)\n",
    "                    all_samples.append({\n",
    "                        'x': x, 'y': y, 'event_date': event_date,\n",
    "                        'match_id': idx, 'label': 0,\n",
    "                        **{FEATURE_NAMES[i]: feature_values[i] for i in range(len(FEATURE_NAMES))}\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nSkipping match {match_row['event_date_str']} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save all samples to CSV\n",
    "samples_df = pd.DataFrame(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3abbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples Generated: 33,368\n",
      "  Mowed (1): 18,077 (54.2%)\n",
      "  Not Mowed (0): 15,291 (45.8%)\n",
      "\n",
      "Saved final samples to: ../data/training_samples_all_indices.csv\n",
      "\n",
      "First 5 samples:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "event_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "match_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ndvi_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "evi_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "savi_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "gndvi_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "ndii_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "ndvi_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "evi_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "savi_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "gndvi_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "ndii_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "blue_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "green_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "red_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "nir_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "swir_diff",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "blue_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "green_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "red_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "nir_after",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "swir_after",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "c9a2da0c-7e3b-46a7-880d-b3451749d798",
       "rows": [
        [
         "0",
         "2684418.974783797",
         "1257664.4868047778",
         "2019-06-23 00:00:00",
         "12",
         "1",
         "0.6056207",
         "2.2761095",
         "0.90832376",
         "0.54775536",
         "0.084742464",
         "-0.25784913",
         "-0.21787696",
         "-0.38676775",
         "-0.22165309",
         "-0.37985122",
         "382.60324",
         "298.4378",
         "444.25665",
         "-1931.239",
         "919.1156",
         "745.6059",
         "993.0122",
         "834.7466",
         "3398.4705",
         "2867.4785"
        ],
        [
         "1",
         "2682788.2668634187",
         "1259725.3814771574",
         "2019-06-23 00:00:00",
         "12",
         "1",
         "0.5519261",
         "1.6887966",
         "0.8277736",
         "0.51119",
         "0.033511348",
         "-0.28606576",
         "-0.72027665",
         "-0.4290783",
         "-0.22685078",
         "-0.40242237",
         "292.29004",
         "258.54474",
         "427.94833",
         "-1470.5127",
         "929.9186",
         "622.58374",
         "899.1569",
         "802.5889",
         "2779.8062",
         "2599.537"
        ],
        [
         "2",
         "2682948.3363525355",
         "1259115.1165498993",
         "2019-06-23 00:00:00",
         "12",
         "1",
         "0.52306205",
         "1.5128255",
         "0.7844996",
         "0.5091372",
         "0.02917052",
         "-0.27462426",
         "-0.7360445",
         "-0.41188803",
         "-0.18902045",
         "-0.3127772",
         "394.64996",
         "365.28674",
         "573.69543",
         "-595.65137",
         "1154.6158",
         "742.7695",
         "1038.8899",
         "1000.1898",
         "3194.024",
         "3012.963"
        ],
        [
         "3",
         "2683208.4492723504",
         "1258164.703958268",
         "2019-06-23 00:00:00",
         "12",
         "1",
         "0.54252905",
         "1.6374009",
         "0.8136992",
         "0.50769776",
         "0.078942135",
         "-0.25496593",
         "-0.56610525",
         "-0.3824062",
         "-0.17675604",
         "-0.301472",
         "406.06848",
         "350.97308",
         "544.1852",
         "-596.7351",
         "1078.0",
         "756.0895",
         "1085.5283",
         "985.9476",
         "3324.4792",
         "2838.0"
        ],
        [
         "4",
         "2683338.505732258",
         "1257924.5997245926",
         "2019-06-23 00:00:00",
         "12",
         "1",
         "0.51905227",
         "1.4729344",
         "0.77848816",
         "0.49716768",
         "0.13170534",
         "-0.23670952",
         "-0.53639066",
         "-0.35501906",
         "-0.1882561",
         "-0.22887266",
         "386.1196",
         "414.8794",
         "526.4359",
         "-396.5691",
         "787.68555",
         "760.186",
         "1100.451",
         "1037.3926",
         "3276.5588",
         "2513.9216"
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>event_date</th>\n",
       "      <th>match_id</th>\n",
       "      <th>label</th>\n",
       "      <th>ndvi_after</th>\n",
       "      <th>evi_after</th>\n",
       "      <th>savi_after</th>\n",
       "      <th>gndvi_after</th>\n",
       "      <th>ndii_after</th>\n",
       "      <th>...</th>\n",
       "      <th>blue_diff</th>\n",
       "      <th>green_diff</th>\n",
       "      <th>red_diff</th>\n",
       "      <th>nir_diff</th>\n",
       "      <th>swir_diff</th>\n",
       "      <th>blue_after</th>\n",
       "      <th>green_after</th>\n",
       "      <th>red_after</th>\n",
       "      <th>nir_after</th>\n",
       "      <th>swir_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.684419e+06</td>\n",
       "      <td>1.257664e+06</td>\n",
       "      <td>2019-06-23</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605621</td>\n",
       "      <td>2.276109</td>\n",
       "      <td>0.908324</td>\n",
       "      <td>0.547755</td>\n",
       "      <td>0.084742</td>\n",
       "      <td>...</td>\n",
       "      <td>382.603241</td>\n",
       "      <td>298.437805</td>\n",
       "      <td>444.256653</td>\n",
       "      <td>-1931.239014</td>\n",
       "      <td>919.115601</td>\n",
       "      <td>745.605896</td>\n",
       "      <td>993.012207</td>\n",
       "      <td>834.746582</td>\n",
       "      <td>3398.470459</td>\n",
       "      <td>2867.478516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.682788e+06</td>\n",
       "      <td>1.259725e+06</td>\n",
       "      <td>2019-06-23</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.551926</td>\n",
       "      <td>1.688797</td>\n",
       "      <td>0.827774</td>\n",
       "      <td>0.511190</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>...</td>\n",
       "      <td>292.290039</td>\n",
       "      <td>258.544739</td>\n",
       "      <td>427.948334</td>\n",
       "      <td>-1470.512695</td>\n",
       "      <td>929.918579</td>\n",
       "      <td>622.583740</td>\n",
       "      <td>899.156921</td>\n",
       "      <td>802.588928</td>\n",
       "      <td>2779.806152</td>\n",
       "      <td>2599.537109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.682948e+06</td>\n",
       "      <td>1.259115e+06</td>\n",
       "      <td>2019-06-23</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523062</td>\n",
       "      <td>1.512825</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>0.509137</td>\n",
       "      <td>0.029171</td>\n",
       "      <td>...</td>\n",
       "      <td>394.649963</td>\n",
       "      <td>365.286743</td>\n",
       "      <td>573.695435</td>\n",
       "      <td>-595.651367</td>\n",
       "      <td>1154.615845</td>\n",
       "      <td>742.769470</td>\n",
       "      <td>1038.889893</td>\n",
       "      <td>1000.189819</td>\n",
       "      <td>3194.023926</td>\n",
       "      <td>3012.962891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.683208e+06</td>\n",
       "      <td>1.258165e+06</td>\n",
       "      <td>2019-06-23</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542529</td>\n",
       "      <td>1.637401</td>\n",
       "      <td>0.813699</td>\n",
       "      <td>0.507698</td>\n",
       "      <td>0.078942</td>\n",
       "      <td>...</td>\n",
       "      <td>406.068481</td>\n",
       "      <td>350.973083</td>\n",
       "      <td>544.185181</td>\n",
       "      <td>-596.735107</td>\n",
       "      <td>1078.000000</td>\n",
       "      <td>756.089478</td>\n",
       "      <td>1085.528320</td>\n",
       "      <td>985.947571</td>\n",
       "      <td>3324.479248</td>\n",
       "      <td>2838.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.683339e+06</td>\n",
       "      <td>1.257925e+06</td>\n",
       "      <td>2019-06-23</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519052</td>\n",
       "      <td>1.472934</td>\n",
       "      <td>0.778488</td>\n",
       "      <td>0.497168</td>\n",
       "      <td>0.131705</td>\n",
       "      <td>...</td>\n",
       "      <td>386.119598</td>\n",
       "      <td>414.879395</td>\n",
       "      <td>526.435913</td>\n",
       "      <td>-396.569092</td>\n",
       "      <td>787.685547</td>\n",
       "      <td>760.185974</td>\n",
       "      <td>1100.451050</td>\n",
       "      <td>1037.392578</td>\n",
       "      <td>3276.558838</td>\n",
       "      <td>2513.921631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x             y event_date  match_id  label  ndvi_after  \\\n",
       "0  2.684419e+06  1.257664e+06 2019-06-23        12      1    0.605621   \n",
       "1  2.682788e+06  1.259725e+06 2019-06-23        12      1    0.551926   \n",
       "2  2.682948e+06  1.259115e+06 2019-06-23        12      1    0.523062   \n",
       "3  2.683208e+06  1.258165e+06 2019-06-23        12      1    0.542529   \n",
       "4  2.683339e+06  1.257925e+06 2019-06-23        12      1    0.519052   \n",
       "\n",
       "   evi_after  savi_after  gndvi_after  ndii_after  ...   blue_diff  \\\n",
       "0   2.276109    0.908324     0.547755    0.084742  ...  382.603241   \n",
       "1   1.688797    0.827774     0.511190    0.033511  ...  292.290039   \n",
       "2   1.512825    0.784500     0.509137    0.029171  ...  394.649963   \n",
       "3   1.637401    0.813699     0.507698    0.078942  ...  406.068481   \n",
       "4   1.472934    0.778488     0.497168    0.131705  ...  386.119598   \n",
       "\n",
       "   green_diff    red_diff     nir_diff    swir_diff  blue_after  green_after  \\\n",
       "0  298.437805  444.256653 -1931.239014   919.115601  745.605896   993.012207   \n",
       "1  258.544739  427.948334 -1470.512695   929.918579  622.583740   899.156921   \n",
       "2  365.286743  573.695435  -595.651367  1154.615845  742.769470  1038.889893   \n",
       "3  350.973083  544.185181  -596.735107  1078.000000  756.089478  1085.528320   \n",
       "4  414.879395  526.435913  -396.569092   787.685547  760.185974  1100.451050   \n",
       "\n",
       "     red_after    nir_after   swir_after  \n",
       "0   834.746582  3398.470459  2867.478516  \n",
       "1   802.588928  2779.806152  2599.537109  \n",
       "2  1000.189819  3194.023926  3012.962891  \n",
       "3   985.947571  3324.479248  2838.000000  \n",
       "4  1037.392578  3276.558838  2513.921631  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Total Samples Generated: {len(samples_df):,}\")\n",
    "print(f\"  Mowed (1): {(samples_df['label'] == 1).sum():,} ({(samples_df['label'] == 1).sum() / len(samples_df) * 100:.1f}%)\")\n",
    "print(f\"  Not Mowed (0): {(samples_df['label'] == 0).sum():,} ({(samples_df['label'] == 0).sum() / len(samples_df) * 100:.1f}%)\")\n",
    "\n",
    "# Save final samples to CSV for model training\n",
    "samples_df.to_csv(final_samples_output_path, index=False)\n",
    "print(f\"\\nSaved final samples to: {final_samples_output_path}\")\n",
    "\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "display(samples_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1e22a",
   "metadata": {},
   "source": [
    "Slight Class imbalance still remains, but it is small enough to be neglected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
